# Feature 0009: Chat Experience Enhancements

## Overview
Enhance the chat interface with improved UX for multilingual support, immediate message display, streaming responses, and proper markdown rendering. Focus on best practices for chat interfaces including RTL/LTR text direction auto-detection, optimistic UI updates, server-sent events for streaming, and styled message rendering.

## Requirements

### 1. Auto-detect RTL/LTR Text Direction for Input
- Chat input field must automatically detect language and align text direction (RTL for Hebrew/Arabic, LTR for others)
- Use character detection to identify RTL languages (Hebrew characters: אבגדהוזחטיכלמנסעפצקרשת, Arabic characters: ا-ي)
- Apply CSS `dir` attribute dynamically to textarea based on first character detection
- Update direction in real-time as user types

### 2. Show User Message Immediately on Send
- Current issue: User message appears only after system response arrives, creating poor UX during "thinking" state
- Display user message optimistically in the conversation immediately when sent
- Show loading/typing indicator separately while waiting for response
- If response fails, handle error gracefully without removing user message

### 3. Implement Streamed Text Response
- Stream assistant responses token-by-token as they're generated
- Use Server-Sent Events (SSE) for real-time streaming from backend
- Update message content progressively in UI as tokens arrive
- Foundation already exists in OpenAI client (supports streaming via AsyncOpenAI)

### 4. Proper Markdown Styling
- Current issue: Raw markdown syntax (##, **, etc.) shows in messages instead of rendered HTML
- Parse and render markdown in assistant messages with proper styling
- Support common markdown: headers, bold, italic, lists, code blocks, links
- Preserve line breaks and formatting in user messages (whitespace-pre-wrap)

## File Changes

### Frontend Changes

#### Message Rendering & Styling

**covenantrix-desktop/package.json**
- Add dependency: `react-markdown` (^9.0.0) for markdown rendering
- Add dependency: `remark-gfm` (^4.0.0) for GitHub Flavored Markdown support

**covenantrix-desktop/src/features/chat/Message.tsx** (NEW)
- Create dedicated Message component for rendering individual messages
- Props: `message: Message`, `isStreaming?: boolean`
- For assistant messages: use `ReactMarkdown` component with custom styling
- For user messages: render plain text with whitespace preservation
- Apply role-specific styling (user: blue bubble, assistant: gray bubble)
- Include timestamp and source citations
- Support partial content rendering during streaming

**covenantrix-desktop/src/features/chat/ChatPanel.tsx**
- Import and use new Message component instead of inline rendering
- Replace lines 81-132 (message rendering logic) with Message component usage
- Update state to track currently streaming message ID
- Add state for partial streaming content: `streamingContent: string`
- Update messages loop to pass `isStreaming` flag to Message component

#### Text Direction Detection

**covenantrix-desktop/src/utils/textDirection.ts** (NEW)
- Create utility functions for text direction detection
- `detectTextDirection(text: string): 'rtl' | 'ltr'`
  - Check first non-whitespace character
  - Hebrew range: \u0590-\u05FF
  - Arabic range: \u0600-\u06FF, \u0750-\u077F, \u08A0-\u08FF
  - Return 'rtl' if first char matches, else 'ltr'
- `isRTLCharacter(char: string): boolean`
- Export both functions

**covenantrix-desktop/src/features/chat/ChatInput.tsx** (NEW)
- Create dedicated ChatInput component (extract from ChatPanel.tsx lines 189-227)
- Props: `onSubmit: (message: string) => void`, `disabled: boolean`
- Add state: `textDirection: 'rtl' | 'ltr'`
- On input change: call `detectTextDirection` and update state
- Apply `dir={textDirection}` attribute to textarea
- Maintain existing features: auto-resize, Shift+Enter for newline, Enter to submit

**covenantrix-desktop/src/features/chat/ChatPanel.tsx** (UPDATE)
- Import and use ChatInput component
- Remove inline input form (lines 189-227)
- Replace with `<ChatInput onSubmit={handleSubmit} disabled={isSubmitting || isTyping} />`

#### Optimistic Message Display

**covenantrix-desktop/src/contexts/ChatContext.tsx**
- Update `sendMessage` function (lines 64-144):
  - Add user message to conversation immediately (before API call)
  - Create temporary message ID for user message
  - Update state with user message before `await chatApi.sendMessage`
  - Add placeholder message for assistant with `isStreaming: true` flag
  - Update assistant message as stream progresses
  - Handle error case: keep user message, show error in assistant message

**covenantrix-desktop/src/types/chat.ts**
- Add optional field to Message interface: `isStreaming?: boolean`
- Add optional field to Message interface: `isError?: boolean`
- Update ChatContextValue to include streaming state

#### Streaming Implementation

**covenantrix-desktop/src/services/api/ChatApi.ts**
- Add new method: `sendMessageStream(request: ChatMessageRequest): AsyncGenerator<string>`
  - Use fetch API with ReadableStream instead of axios
  - Endpoint: `/chat/message/stream`
  - Parse SSE format: `data: {json}\n\n`
  - Yield token strings as they arrive
  - Handle connection errors and cleanup

**covenantrix-desktop/src/contexts/ChatContext.tsx**
- Update `sendMessage` to use streaming:
  - Call `chatApi.sendMessageStream` instead of `sendMessage`
  - Iterate through async generator with `for await`
  - Accumulate streaming content in state
  - Update assistant message content progressively
  - Mark message as complete when stream ends
  - Fall back to non-streaming if streaming fails

### Backend Changes

#### Streaming Endpoint

**backend/api/routes/chat.py**
- Add new endpoint: `POST /chat/message/stream`
  - Accept same ChatMessageRequest schema
  - Return `StreamingResponse` with `media_type="text/event-stream"`
  - Call `service.send_message_stream` generator
  - Format each token as SSE: `f"data: {json.dumps({'token': token})}\n\n"`
  - Send final event with complete response and sources: `f"data: {json.dumps({'done': True, 'message_id': id, 'sources': sources})}\n\n"`

**backend/api/schemas/chat.py**
- Add schema: `StreamTokenResponse(BaseModel)`
  - Fields: `token: Optional[str]`, `done: bool`, `message_id: Optional[str]`, `sources: Optional[List[dict]]`

#### Streaming Service Logic

**backend/domain/chat/service.py**
- Add method: `async def send_message_stream(message: str, conversation_id: Optional[str], agent_id: Optional[str], document_ids: Optional[List[str]]) -> AsyncGenerator[str, None]`
  - Similar flow to `send_message` but yield tokens instead of returning complete response
  - Get/create conversation
  - Add user message immediately
  - Call streaming RAG or agent method
  - Yield each token as it arrives
  - After stream completes: save conversation with complete assistant message
  - Raise exceptions same as non-streaming version

**backend/domain/chat/models.py**
- No changes needed (ChatResponse already supports sources)

#### RAG Engine Streaming

**backend/infrastructure/ai/rag_engine.py**
- Add method: `async def query_stream(query: str, mode: Optional[str], top_k: Optional[int]) -> AsyncGenerator[str, None]`
  - Similar to `query` method but stream tokens
  - Modify `_create_llm_func` to support streaming:
    - Check if `streaming=True` in kwargs (pass through from LightRAG)
    - When streaming: call `client.chat.completions.create(..., stream=True)`
    - Iterate through stream chunks: `async for chunk in response`
    - Extract delta content: `chunk.choices[0].delta.content`
    - Yield each content piece
  - Create separate streaming LLM function: `_create_streaming_llm_func`
  - Use QueryParam with streaming parameter
  - Yield tokens as they arrive from OpenAI

**backend/infrastructure/ai/openai_client.py**
- No changes needed if using AsyncOpenAI directly in rag_engine
- Streaming already supported by OpenAI SDK

#### Language Response Alignment

**backend/infrastructure/ai/rag_engine.py**
- Update `query` and `query_stream` methods:
  - Already has `get_effective_language` method (lines 432-456)
  - Ensure language detection is applied to responses
  - Pass detected language in system prompt to LLM for response language matching
  - Update system prompt in `_create_llm_func` to include: "Respond in the same language as the user's query."

### Styling Updates

**covenantrix-desktop/src/styles/markdown.css** (NEW)
- Create dedicated stylesheet for markdown rendering in messages
- Style headers (h1-h6) with appropriate sizing and spacing
- Style code blocks with monospace font and background
- Style inline code with subtle background
- Style lists (ul, ol) with proper indentation
- Style blockquotes with left border
- Style links with blue color and hover effect
- Style tables if needed
- Ensure styles work with both light and dark themes

**covenantrix-desktop/src/main.tsx**
- Import markdown styles: `import './styles/markdown.css'`

## Implementation Approach

### Step 1: Text Direction Detection
1. Create `textDirection.ts` utility with detection functions
2. Create `ChatInput.tsx` component with RTL/LTR support
3. Update ChatPanel to use ChatInput component
4. Test with Hebrew and Arabic input

### Step 2: Optimistic Message Display
1. Update ChatContext to add user message before API call
2. Add temporary streaming placeholder for assistant message
3. Update ChatPanel to show both messages during loading
4. Test message display timing

### Step 3: Message Rendering Component
1. Install react-markdown and remark-gfm packages
2. Create markdown.css stylesheet
3. Create Message.tsx component with markdown rendering
4. Update ChatPanel to use Message component
5. Test markdown rendering with various formats

### Step 4: Backend Streaming Support
1. Add streaming LLM function to RAG engine
2. Add `query_stream` method to RAG engine
3. Add `send_message_stream` method to ChatService
4. Add streaming endpoint to chat routes
5. Test backend streaming independently

### Step 5: Frontend Streaming Integration
1. Add `sendMessageStream` method to ChatApi
2. Update ChatContext to consume streaming API
3. Update Message component to handle partial content
4. Test end-to-end streaming
5. Add error handling for stream interruptions

### Step 6: Language Response Alignment
1. Update RAG engine system prompts to include language matching instruction
2. Leverage existing `get_effective_language` method
3. Test with multilingual queries

## Testing Considerations
- Test RTL input with Hebrew and Arabic text
- Test LTR input with English, Spanish, French
- Test mixed-direction text (numbers in RTL text)
- Test optimistic UI with slow network conditions
- Test streaming with various message lengths
- Test markdown rendering with all supported formats
- Test streaming interruption and reconnection
- Test error handling when streaming fails
- Verify markdown escaping prevents XSS
- Test dark mode compatibility for all styles

