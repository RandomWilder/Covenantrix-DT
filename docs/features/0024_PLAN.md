# Feature 0024: User-Selectable LLM Models for RAG Generation

## Overview
Add ability for users to select the OpenAI LLM model used for RAG response generation through the Settings â†’ RAG Config tab. Currently, the system hardcodes `gpt-4o-mini` in the RAG engine. This feature will expose GPT-5 series, GPT-4o series, and GPT-4 Turbo models as user-configurable options, with `gpt-5-nano-2025-08-07` as the new default.

The selected model directly affects RAG query responses in both streaming and non-streaming modes. Model selection persists in user settings and applies immediately when settings are reloaded (no app restart required).

## Model List
Based on official OpenAI API verification (ran `list_openai_models.py`), the following models are available and will be exposed:

```python
class LLMModel(str, Enum):
    """Available OpenAI models for RAG generation"""
    
    # Premium (Latest)
    GPT_5_PRO = "gpt-5-pro-2025-10-06"
    GPT_5 = "gpt-5-2025-08-07"
    GPT_5_MINI = "gpt-5-mini-2025-08-07"
    GPT_5_NANO = "gpt-5-nano-2025-08-07"  # Default
    
    # Standard (Current Production)
    GPT_4O = "gpt-4o"
    GPT_4O_MINI = "gpt-4o-mini"
    GPT_4_TURBO = "gpt-4-turbo"
```

## Files to Modify

### Backend - Schema Layer
**File: `backend/api/schemas/settings.py`**
- Add `LLMModel` enum (lines will be added after line 22, before `ApiKeySettings`)
- Update `RAGSettings` class (line 74-80):
  - Add new field: `llm_model: LLMModel = Field(default=LLMModel.GPT_5_NANO)`
  - Keep existing fields: `search_mode`, `top_k`, `use_reranking`, `enable_ocr`

### Backend - RAG Engine Layer
**File: `backend/infrastructure/ai/rag_engine.py`**

1. **Initialization** (line 40-76):
   - Store user settings in `__init__` (already done at line 74)
   - Extract model setting during initialization

2. **Non-Streaming LLM Function** (line 99-158, `_create_llm_func`):
   - Currently hardcodes `model="gpt-4o-mini"` at line 151
   - Change to read from user settings: `self.user_settings.get("rag", {}).get("llm_model", "gpt-5-nano-2025-08-07")`
   - Model string is passed directly to `client.chat.completions.create()`

3. **Streaming LLM Function** (line 160-224, `_create_streaming_llm_func`):
   - Currently hardcodes `model="gpt-4o-mini"` at line 213
   - Change to read from user settings (same pattern as non-streaming)

4. **Apply Settings Method** (line 543-581, `apply_settings`):
   - Already reads `rag_settings` at line 554
   - Add extraction and storage of `llm_model` setting for easier access
   - Store as instance variable: `self.llm_model = rag_settings.get("llm_model", "gpt-5-nano-2025-08-07")`

5. **Status Method** (line 609-625, `get_status`):
   - Add `llm_model` to returned status dict (after line 622)

### Backend - OpenAI Client (Optional Enhancement)
**File: `backend/infrastructure/ai/openai_client.py`**
- Line 64: `self.default_model = "gpt-4o-mini"` 
- Consider updating default to `"gpt-5-nano-2025-08-07"` for consistency
- Note: This class is used for structured extraction, not RAG generation

### Frontend - Type Definitions
**File: `covenantrix-desktop/src/types/settings.ts`**

1. Add new type (after line 12, before `ApiKeySettings`):
```typescript
export type LLMModel = 
  | 'gpt-5-pro-2025-10-06'
  | 'gpt-5-2025-08-07'
  | 'gpt-5-mini-2025-08-07'
  | 'gpt-5-nano-2025-08-07'
  | 'gpt-4o'
  | 'gpt-4o-mini'
  | 'gpt-4-turbo';
```

2. Update `RAGSettings` interface (line 21-26):
   - Add new field: `llm_model: LLMModel;`

### Frontend - RAG Tab UI
**File: `covenantrix-desktop/src/features/settings/RAGTab.tsx`**

1. **Add Icon Import** (line 7):
   - Add `Brain` icon to imports from `lucide-react`

2. **Add Model Selection Section** (insert between line 125 and line 127, after "Search Mode" section):
   - New section with `Brain` icon
   - Dropdown/select element with all 7 models
   - User-friendly labels:
     - "GPT-5 Pro (Premium, Most Capable)" - `gpt-5-pro-2025-10-06`
     - "GPT-5 (Latest, High Performance)" - `gpt-5-2025-08-07`
     - "GPT-5 Mini (Balanced, Recommended)" - `gpt-5-mini-2025-08-07`
     - "GPT-5 Nano (Fast, Cost-Effective, Default)" - `gpt-5-nano-2025-08-07`
     - "GPT-4o (Multimodal, Current Production)" - `gpt-4o`
     - "GPT-4o Mini (Fast, Efficient)" - `gpt-4o-mini`
     - "GPT-4 Turbo (Stable, Enhanced)" - `gpt-4-turbo`
   - onChange handler: `onChange({ ...settings, llm_model: e.target.value as LLMModel })`

3. **Update Performance Tips** (line 222-237):
   - Add tip about model selection impact on speed/cost/quality

### Frontend - Settings Modal
**File: `covenantrix-desktop/src/features/settings/SettingsModal.tsx`**
- Line 344: Default RAG settings object
- Add `llm_model: 'gpt-5-nano-2025-08-07'` to default settings

### Frontend - Settings Storage
**File: `covenantrix-desktop/src/services/storage/SettingsStorage.ts`**
- Verify default settings initialization includes new `llm_model` field
- Schema validation should accept the new field

## Data Flow

### Settings Save Flow
1. User selects model in RAG Tab dropdown
2. `RAGTab.onChange()` updates local settings state
3. User clicks "Save" in Settings Modal
4. `POST /api/settings` with updated settings including `rag.llm_model`
5. Backend validates and saves to `user_settings_storage.py`
6. Settings persisted to `backend/storage/user_settings.json`

### Settings Apply Flow
1. User clicks "Apply" in Settings Modal
2. `POST /api/settings/apply` called
3. Backend route handler: `backend/api/routes/settings.py` line 413-482 (`apply_settings`)
4. Calls `reload_rag_with_settings()` (line 355-410)
5. Creates new RAG engine with fresh settings
6. RAG engine reads `llm_model` from settings during LLM function creation
7. Model applied to all subsequent queries

### Query Execution Flow
1. User submits query via chat
2. RAG engine's `query()` or `query_stream()` method called
3. LLM function (`_create_llm_func` or `_create_streaming_llm_func`) invoked
4. Function reads `self.llm_model` or extracts from `self.user_settings`
5. OpenAI API called with selected model string
6. Response generated using chosen model

## Technical Notes

### Model String Format
- OpenAI API expects exact model ID strings (e.g., `"gpt-5-nano-2025-08-07"`)
- No validation needed beyond enum - if model becomes unavailable, OpenAI API returns error
- Error handling already exists in RAG engine (lines 456-464 in `query` method)

### Default Behavior
- New users: Get `gpt-5-nano-2025-08-07` as default
- Existing users: Settings migration not required - will fall back to `gpt-5-nano-2025-08-07` if field missing
- Both frontend and backend have fallback defaults in multiple locations

### No Breaking Changes
- Additive change only - adds new optional field
- Existing settings remain compatible
- No database migrations needed (JSON storage)
- No API contract changes

### Performance Implications
- Different models have different:
  - Speed: Nano/Mini fastest, Pro slowest
  - Cost: Nano cheapest, Pro most expensive
  - Quality: Pro highest quality, Nano good for simple queries
  - Context window: Pro has largest context window
- Model selection immediately affects next query (no caching of old model)

## Separation of Concerns Compliance

**Schema Layer** (`backend/api/schemas/settings.py`)
- Defines valid models and data structure
- Validates enum values
- No business logic

**Storage Layer** (`backend/infrastructure/storage/user_settings_storage.py`)
- Already handles settings persistence
- No changes needed - stores arbitrary JSON

**API Layer** (`backend/api/routes/settings.py`)
- Settings routes handle save/apply
- No changes needed - already forwards all settings

**Service Layer** (`backend/infrastructure/ai/rag_engine.py`)
- RAG engine reads model from settings
- Applies model to OpenAI API calls
- Contains generation logic

**UI Layer** (`covenantrix-desktop/src/features/settings/RAGTab.tsx`)
- Presents model options to user
- Captures user selection
- No business logic

**Type Layer** (`covenantrix-desktop/src/types/settings.ts`)
- Defines TypeScript types matching backend schema
- Type safety only

## Testing Considerations

1. **Settings Persistence**: Verify model selection saves and loads correctly
2. **Settings Application**: Verify RAG engine reloads with new model
3. **Query Generation**: Test queries with different models return responses
4. **Error Handling**: Test with invalid model name (should fall back to default)
5. **Default Behavior**: Test fresh install gets correct default model
6. **Migration**: Test existing settings files work without `llm_model` field

